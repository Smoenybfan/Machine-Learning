#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Zusammenfassung Machine Learning HS 17
\end_layout

\begin_layout Section
Supervised Learning
\end_layout

\begin_layout Standard
Given some data, we want to be able to predict the outcome.
 E.g.
 when we have the Living area, we may want to predict the price of a house
 as a function of the size of the living area.
 
\end_layout

\begin_layout Standard
To establish a notation, we'll use 
\begin_inset Formula $x^{(i)}$
\end_inset

 to denote the input variables (the living area in our example).
 These are also called 
\series bold
input features
\series default
.
 We'll use 
\begin_inset Formula $y^{(i)}$
\end_inset

 to denote the output, which is also called 
\series bold
target variable
\series default
.
 A pair 
\begin_inset Formula $(x^{(i)},y^{(i)})$
\end_inset

 is then called a 
\series bold
training example 
\series default
and the whole list of training examples we're using is called the 
\series bold
training set
\series default
.
 The i says that this is the i-th training example.
 
\end_layout

\begin_layout Standard
Our goal is, given a training set, to find a function 
\begin_inset Formula $h$
\end_inset

 so that 
\begin_inset Formula $h(x)$
\end_inset

 predicts 
\begin_inset Formula $y$
\end_inset

 
\begin_inset Quotes eld
\end_inset

accuratly
\begin_inset Quotes erd
\end_inset

.
 This function 
\begin_inset Formula $h(x)$
\end_inset

 is called a 
\series bold
hypothesis
\series default
.
 
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename ../../Machine Learning/hypothesis.png

\end_inset


\end_layout

\begin_layout Standard
When the target variable that we're trying to predict is continuous (can
 take on infinitely many, uncountable values), we call the learning problem
 a 
\series bold
regression problem
\series default
.
 When 
\begin_inset Formula $y$
\end_inset

 can take on only a small number of discrete values, we call it a 
\series bold
classification problem.
 
\end_layout

\begin_layout Subsection
Linear Regression
\end_layout

\begin_layout Standard
To perform supervised learning, we must decide how to represent our hypothesis
 
\begin_inset Formula $h$
\end_inset

.
 As an inital choice, we're going to approximate 
\begin_inset Formula $y$
\end_inset

 as a linear function of 
\begin_inset Formula $x$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{\Theta}(x)=\sum_{i=0}^{n}\Theta_{i}x_{i}=\Theta^{T}x
\]

\end_inset


\begin_inset Newline newline
\end_inset

The 
\begin_inset Formula $\Theta_{i}$
\end_inset

's are the 
\series bold
parameters
\series default
 or the 
\series bold
weights
\series default
 which parameterize our functions mapping from the 
\series bold
feature space 
\begin_inset Formula $X$
\end_inset

 
\series default
to the 
\series bold
target space
\series default
 
\begin_inset Formula $Y$
\end_inset

.
 Note, that 
\begin_inset Formula $x_{0}$
\end_inset

(also called the 
\series bold
intercept term
\series default
) is always equal to 1 to simplify our notation.
 On the right-hand side of the equation above, we use both 
\begin_inset Formula $\Theta$
\end_inset

and 
\begin_inset Formula $x$
\end_inset

 as vectors to simplify the math.
 
\end_layout

\begin_layout Standard
Now how are we going to learn our parameters 
\begin_inset Formula $\Theta$
\end_inset

? We will try to make 
\begin_inset Formula $h(x)$
\end_inset

 to be as close to 
\begin_inset Formula $y$
\end_inset

 as possible for the training examples we have.
 To do this, we define the 
\series bold
cost function
\series default
, which measures how close the 
\begin_inset Formula $h(x^{(i)})$
\end_inset

's are to the corresponding 
\begin_inset Formula $y^{(i)}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J(\Theta)=\frac{1}{2}\sum_{i=1}^{m}(h_{\Theta}(x^{(i)})-y^{(i)})^{2}
\]

\end_inset


\end_layout

\begin_layout Subsubsection
LMS algorithm
\end_layout

\begin_layout Standard
We want to choose 
\begin_inset Formula $\Theta$
\end_inset

so as to make 
\begin_inset Formula $J(\Theta)$
\end_inset

 as small as possible.
 We start by choosing some initial guess for 
\begin_inset Formula $\Theta$
\end_inset

and then we repeatedly change 
\begin_inset Formula $\Theta$
\end_inset

to minimize 
\begin_inset Formula $J(\Theta)$
\end_inset

 step by step.
 We now look at the 
\series bold
gradient descent 
\series default
algorithm that does exactly that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta_{j}:=\Theta_{j}-\alpha\frac{\partial}{\partial}J(\Theta)
\]

\end_inset

where 
\begin_inset Formula $\alpha$
\end_inset

is the 
\series bold
learning rate
\series default
.
 This way w're going stepwise in the direction of steepest descrease of
 J.
 For a single training example, we get the update rule:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta_{j}:=\Theta_{j}+\alpha(y^{(i)}-h_{\Theta}(x^{(i)}))x_{j}^{(i)}
\]

\end_inset

This rule is called the 
\series bold
LMS (least mean square)
\series default
 update rule.
 It is proportional to the error term 
\begin_inset Formula $(y^{(i)}-h_{\Theta}(x^{(i)}))$
\end_inset

 so for small errors w're only going to make small adjustments whereas we're
 going to make bigger adjustments for bigger errors.
\end_layout

\begin_layout Standard
The adjustment to the whole training set is quite simple:
\end_layout

\begin_layout Standard
\begin_inset space \qquad{}
\end_inset

Loop {
\end_layout

\begin_layout Standard
\begin_inset space \qquad{}
\end_inset


\begin_inset space \qquad{}
\end_inset

for i=1 to m, {
\end_layout

\begin_layout Standard
\begin_inset space \qquad{}
\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset space \qquad{}
\end_inset


\begin_inset Formula $\Theta_{j}:=\Theta_{j}+\alpha(y^{(i)}-h_{\Theta}(x^{(i)}))x_{j}^{(i)}$
\end_inset

 (for every j)
\end_layout

\begin_layout Standard
\begin_inset space \qquad{}
\end_inset


\begin_inset space \qquad{}
\end_inset

}
\end_layout

\begin_layout Standard
\begin_inset space \qquad{}
\end_inset

}
\end_layout

\begin_layout Standard
We repeatedly run through the training set and for each training example
 we update all he parameters.
 This algorithm is called 
\series bold
stochastic gradient descent 
\series default
or 
\series bold
incremental gradient descent
\series default
.
 
\end_layout

\begin_layout Standard
Another option is the 
\series bold
batch gradient descent
\series default
:
\end_layout

\begin_layout Standard
\begin_inset space \qquad{}
\end_inset

Repeat until convergence{
\begin_inset Formula 
\[
\Theta_{j}:=\Theta_{j}+\alpha\sum_{i=1}^{m}(y^{(i)}-h_{\Theta}(x^{(i)}))x_{j}^{(i)}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset space \qquad{}
\end_inset

}
\end_layout

\begin_layout Standard
Stochastic gradient is often preferred to batch gradient because whereas
 batch gradient has to scan over the whole training set (which can get costly
 for large m), stochastic gradient can start right away with the first training
 example.
 Note, that stochastic gradient doesn't reach a global minimum (as batch
 gradient) but oscillates around an optimal solution.
 
\end_layout

\begin_layout Subsection
The normal equations
\end_layout

\begin_layout Standard
We will now see another way to minimize 
\begin_inset Formula $J$
\end_inset

.
 In this method we' re going to take the derivatives of 
\begin_inset Formula $J$
\end_inset

 with respect to the 
\begin_inset Formula $\Theta_{j}$
\end_inset

's and set them to zero.
 First we're repeating some rules to simplify our math.
\end_layout

\begin_layout Subsubsection
Matrix derivatives
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{A}f(A)=\left[\begin{array}{ccc}
\frac{\partial f}{\partial A_{11}} & \dots & \frac{\partial f}{\partial A_{1n}}\\
\vdots & \ddots & \vdots\\
\frac{\partial f}{\partial A_{m1}} & \dots & \frac{\partial f}{\partial A_{mn}}
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
tr(A)=tr(A^{T})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
tr(ABCD)=tr(DABC)=tr(CDAB)=tr(BCDA)
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Least squares revisited
\end_layout

\begin_layout Standard
Given a training set, we define the 
\series bold
design matrix
\series default
 X to be the matrix containing all training examples, where the i-th training
 example is the i-th row.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X=\left[\begin{array}{c}
(x^{(1)})^{T}\\
\vdots\\
(x^{(m)})^{T}
\end{array}\right]
\]

\end_inset

Let 
\begin_inset Formula $\overrightarrow{y}$
\end_inset

be the m-dimensional vector containing all target values from the training
 set.
 Using math rules and the formula we got in the last section we get
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\nabla_{\Theta}J(\Theta)=X^{T}X\Theta-X^{T}y
\]

\end_inset

Therefore the value of 
\begin_inset Formula $\Theta$
\end_inset

that minimizes 
\begin_inset Formula $J(\Theta)$
\end_inset

 is given by the equation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta=(X^{T}X)^{-1}X^{T}y
\]

\end_inset


\end_layout

\begin_layout Subsection
Probabilistic interpretation
\end_layout

\begin_layout Standard
We will now look at linear regression under terms of probabilistic assumptions.
 To start, we assume that the target variable and the inputs are related
 via
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y^{(i)}=\Theta^{T}x^{(i)}+\epsilon^{(i)}
\]

\end_inset

where 
\begin_inset Formula $\epsilon^{(i)}$
\end_inset

 is an error term that is distributed IID (independently and identically
 distributed) according to a Gaussian distribution with mean 0 and variance
 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 We write this as 
\begin_inset Quotes eld
\end_inset


\begin_inset Formula $\epsilon^{(i)}\sim N(0,\sigma^{2})."$
\end_inset

 This implies that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(y^{(i)}|\,x^{(i)};\Theta)=\frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{(y^{(i)}-\Theta^{T}x^{(i)})^{2}}{2\sigma^{2}}\right)
\]

\end_inset

This follows from the densitiy of 
\begin_inset Formula $\epsilon^{(i)}$
\end_inset

.
 The notation 
\begin_inset Formula $p(y^{(i)}|\:x^{(i)};\Theta)$
\end_inset

 indicates the distribution of 
\begin_inset Formula $y^{(i)}$
\end_inset

given 
\begin_inset Formula $x^{(i)}$
\end_inset

, parameterized by 
\begin_inset Formula $\Theta$
\end_inset

.
 Getting back to our whole training set we can also write the probability
 of the whole data as 
\begin_inset Formula $p(y|\:X;\Theta)$
\end_inset

.
 When we wish to see this as a function of 
\begin_inset Formula $\Theta$
\end_inset

, we will call it the 
\series bold
likelihood
\series default
 function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(\Theta)=L(\Theta;X,y)=p(y|\,X;\Theta)
\]

\end_inset

Because of our assumption of independence for the 
\begin_inset Formula $\epsilon^{(i)}$
\end_inset

, this can also be written as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(\Theta)=\prod_{i=1}^{m}p(y^{(i)}|\:x^{(i)};\Theta)=\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp\left(-\frac{(y^{(i)}-\Theta^{T}x^{(i)})^{2}}{2\sigma^{2}}\right)
\]

\end_inset

The principal of 
\series bold
maximum likelihood
\series default
 says that we should choose 
\begin_inset Formula $\Theta$
\end_inset

so as to make the data as high probability as possible.
 Therefore we should choose 
\begin_inset Formula $\Theta$
\end_inset

to maximize 
\begin_inset Formula $L(\Theta)$
\end_inset

.
 To make our calculations simpler we will instead of maximizing 
\begin_inset Formula $L(\Theta)$
\end_inset

maximize a strictly increasing function of 
\begin_inset Formula $L(\Theta)$
\end_inset

, in our case the 
\series bold
log likelihood
\series default
 
\begin_inset Formula $l(\Theta)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
l(\Theta)=log\,L(\Theta)=m\,log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma^{2}}\cdot\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\Theta^{T}x^{(i)})^{2}
\]

\end_inset

Hence, maximizing 
\begin_inset Formula $l(\Theta)$
\end_inset

gives the same answer as maximizing 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\Theta^{T}x^{(i)})^{2},
\]

\end_inset

which is our original 
\begin_inset Formula $J(\Theta)$
\end_inset

, the cost function from least-square.
 
\end_layout

\begin_layout Section
Classification and logistic regression
\end_layout

\begin_layout Standard
Classification is quite similar to the regression problem, but now our values
 
\begin_inset Formula $y$
\end_inset

 take on only a small number of discrete values.
 To start, we will focus on the 
\series bold
inary classification
\series default
 problem, where 
\begin_inset Formula $y$
\end_inset

 can only take on 1 and 0.
 Given 
\begin_inset Formula $x^{(i)}$
\end_inset

, the corresponding 
\begin_inset Formula $y^{(i)}$
\end_inset

is also called the 
\series bold
label
\series default
 for the training example.
 
\end_layout

\begin_layout Subsection
Logistic regression
\end_layout

\begin_layout Standard
If we would use our already known linear regression algorithm to predict
 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

, we would see that this method performs rather poorly.
 It also makes no sense that 
\begin_inset Formula $h_{\Theta}(x)$
\end_inset

 can take on values outside of 
\begin_inset Formula $[0,1]$
\end_inset

.
 To fix this we will change our hypothesis:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{\Theta}(x)=g(\Theta^{T}x)=\frac{1}{1+e^{-\Theta^{T}x}}
\]

\end_inset

where
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g(z)=\frac{1}{1+e^{-z}}
\]

\end_inset

is called the 
\series bold
logistic function
\series default
 or the 
\series bold
sigmoid function
\series default
.
 As you can see, 
\begin_inset Formula $g(z)$
\end_inset

 tends to 1 for large values of z and to 0 for small values of z.
 The derivative of the sigmoid function is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g'(z)=g(z)(1-g(z)).
\]

\end_inset


\end_layout

\begin_layout Standard
Similar as before we will now make a set of probabilistic assumptions and
 then fit our parameters via maximum lieklihood.
 Let us assume that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(y=1|\,x;\Theta)=h_{\Theta}(x)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(y=0|\,x;\Theta)=1-h_{\Theta}(x)
\]

\end_inset

 Therefore
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(y|x;\Theta)=(h_{\Theta}(x))^{y}(1-h_{\Theta}(x))^{1-y}
\]

\end_inset

Again, assuming that our 
\begin_inset Formula $m$
\end_inset

 training examples ar IID, the likelihood of the parameters is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(\Theta)=\prod_{i=1}^{m}(h_{\Theta}(x^{(i)}))^{y^{(i)}}(1-h_{\Theta}(x^{(i)}))^{1-y^{(i)}}
\]

\end_inset

As before, we will again maximize the log likelihood
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
l(\Theta)=log\,L(\Theta)=\sum_{i=1}^{m}y^{(i)}log\,h(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))
\]

\end_inset


\end_layout

\begin_layout Standard
How do we maximize the likelihood? Similar as with linear regression we
 can use gradient ascent.
 In vectorial notation, our updates will be given by 
\begin_inset Formula $\Theta:=\Theta+\alpha\nabla_{\Theta}l(\Theta)$
\end_inset

 where 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial}{\partial\Theta_{j}}l(\Theta)=(y-h_{\Theta}(x))x_{j}
\]

\end_inset

This gives us therefore the stochastic gradient ascent rule
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta_{j}:=\Theta_{j}+\alpha(y^{(i)}-h_{\Theta}(x^{(i)}))x_{j}^{(i)}
\]

\end_inset


\end_layout

\begin_layout Subsection
The perceptron learning algorithm
\end_layout

\begin_layout Standard
We want to be able to 
\begin_inset Quotes eld
\end_inset

force
\begin_inset Quotes erd
\end_inset

 our algorithm to output values that are either exactly 1 or 0.
 We can do this by modifying 
\begin_inset Formula $g(z)$
\end_inset

so that it's outputing 1 if 
\begin_inset Formula $z\ge0$
\end_inset

 and 0 if 
\begin_inset Formula $z<0$
\end_inset

.
 If we then let 
\begin_inset Formula $h_{\Theta}(x)=g(\Theta^{T}x)$
\end_inset

 and use the same update rule as above, we have the 
\series bold
perceptron learning algorithm
\series default
.
 
\end_layout

\begin_layout Subsection
Another algorithm for maximizing 
\begin_inset Formula $l(\Theta)$
\end_inset


\end_layout

\begin_layout Standard
First, consider Newton's method for finding a zero of a function.
 The update rule here is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta:=\Theta-\frac{f(\Theta)}{f'(\Theta)}
\]

\end_inset

Here, we're approximating the minimum by taking the tangent of the function,
 going through the current guess of 
\begin_inset Formula $\Theta$
\end_inset

.
 The point where this tangent is zero becomes our new guess.
 
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename newton_graphs.png

\end_inset


\end_layout

\begin_layout Standard
This method gives us a way to get to 
\begin_inset Formula $f(\Theta)=0.$
\end_inset

 But we want to use it to maximize a function.
 The maxima of this function 
\begin_inset Formula $l$
\end_inset

 correspond to points where the first derivative is zero.
 Therefore we obtain the update rule
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta:=\Theta-\frac{l'(\Theta)}{l''(\Theta)}
\]

\end_inset

The generalization if this method to the multidimensional setting is given
 by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Theta:=\Theta-H^{-1}\nabla_{\Theta}l(\Theta)
\]

\end_inset

where 
\begin_inset Formula $H$
\end_inset

 is the Hessian which is given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H_{ij}=\frac{\partial^{2}l(\Theta)}{\partial\Theta_{i}\partial\Theta_{j}}.
\]

\end_inset

 Usually, Newton's method converges much faster than gradiant descent and
 needs fewer iterations.
 But for large numbers of features this method can get very costly because
 the inverse of a big matrix needs to be calulated.
 When Newton's method is applied to maximize the logistic regression log
 likelihood function 
\begin_inset Formula $l(\Theta)$
\end_inset

, the resulating method is also called 
\series bold
Fisher scoring
\series default
.
 
\end_layout

\begin_layout Section
Generative Learning algorithms
\end_layout

\begin_layout Standard
So far we always modelled 
\begin_inset Formula $p(y|x)$
\end_inset

.
 Now we want to learn 
\begin_inset Formula $p(x|y)$
\end_inset

.
 For example, if 
\begin_inset Formula $y$
\end_inset

 indicates whetere an example is a dog (0) or an elephant (1), then 
\begin_inset Formula $p(x|y=0)$
\end_inset

 models the distribution of the dogs' features.
 After modeling 
\begin_inset Formula $p(y)$
\end_inset

 (called 
\series bold
class priors
\series default
) and 
\begin_inset Formula $p(x|y)$
\end_inset

 our algorithm can then use Bayes rule to derive the posterior distribution
 on 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(y|x)=\frac{p(x|y)p(y)}{p(x)}
\]

\end_inset


\end_layout

\begin_layout Subsection
Gaussian discriminant analysis GDA
\end_layout

\begin_layout Standard
This model assumes that 
\begin_inset Formula $p(x|y)$
\end_inset

 is distributed according to a multivariate normal distribution.
 
\end_layout

\begin_layout Subsubsection
Multivariate Normal Distribution
\end_layout

\begin_layout Standard
The multivariate normal distribution in 
\begin_inset Formula $n$
\end_inset

-dimensions is parameterized by a 
\series bold
mean vector
\series default
 
\begin_inset Formula $\mu\in\mathbb{{R}}^{n}$
\end_inset

and a 
\series bold
covariance matrix 
\series default

\begin_inset Formula $\Sigma\in\mathbb{R}^{n\,x\,n}$
\end_inset

 where 
\begin_inset Formula $\Sigma$
\end_inset

is symmetric and positive definitve.
 The density (also written as 
\begin_inset Formula $N(\mu,\Sigma)$
\end_inset

) is given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu))
\]

\end_inset

The mean of this density is of course 
\begin_inset Formula $\mu$
\end_inset

.
 The 
\series bold
covariance
\series default
 of a vector-valued random variable 
\begin_inset Formula $Z$
\end_inset

 is defined as Cov(
\begin_inset Formula $Z$
\end_inset

)=E[(Z-E[Z])(Z-E[Z])
\begin_inset Formula $^{T}$
\end_inset

] If X is distributed according to a multivariate normal distribution, then
 
\begin_inset Formula $Cov(X)=\Sigma$
\end_inset

.
\end_layout

\begin_layout Standard
Graphically spoken, the covariance matrix changes the shape of the ellipsoid
 where mean moves the center:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename vary_sigma.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename vary_mu.png

\end_inset


\end_layout

\begin_layout Subsubsection
The Gaussian Discriminant Analysis model
\end_layout

\begin_layout Standard
The model is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y\sim Bernoulli(\phi)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x|y=0\sim N(\mu_{0},\Sigma)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x|y=1\sim N(\mu_{1},\Sigma)
\]

\end_inset

The parameters of our model here are 
\begin_inset Formula $\phi,\Sigma,\mu_{0}$
\end_inset

and 
\begin_inset Formula $\mu_{1}$
\end_inset

.The log-likelihood of the data is given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
l(\phi,\mu_{0,}\mu_{1},\Sigma)=log\prod_{i=1}^{m}p(x^{(i)},y^{(i)};\phi,\mu_{0},\mu_{1},\Sigma)
\]

\end_inset

When we maximize 
\begin_inset Formula $l$
\end_inset

 with respect to the parameters we find he maximum likelihood estimates
 of the parameters to be:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi=\frac{1}{m}\sum_{i=1}^{m}1\{y^{(i)}=1\}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mu_{0}=\frac{\sum_{i=1}^{m}1\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^{m}1\{y^{(i)}=0\}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mu_{1}=\frac{\sum_{i=1}^{m}1\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^{m}1\{y^{(i)}=1\}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Sigma=\frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-u_{y^{(i)}})^{T}
\]

\end_inset

The following image shows what the algorithm is doing:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename gda.png

\end_inset


\end_layout

\begin_layout Standard
The data has been fit to the two gaussians we see here.
 One is for the data that is an x, the other is for the data that is a circle.
 Also shown is the decision boundardy where 
\begin_inset Formula $p(y=1|x)=0.5$
\end_inset

.
 On one side we predict 
\begin_inset Formula $y=1$
\end_inset

, on the other we predict 
\begin_inset Formula $y=0$
\end_inset

.
 The gaussians have the same shape because they have the same covariance
 matrix but their mean 
\begin_inset Formula $\mu_{0},\mu_{1}$
\end_inset

is different .
 
\end_layout

\begin_layout Subsubsection
Discussion: GDA and logistic regression
\end_layout

\begin_layout Standard
We can view 
\begin_inset Formula $p(y=1|x;\phi,\mu_{0},\mu_{1},\Sigma)$
\end_inset

 as a function of x and will find that in can be expressed in the form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(y=1|x;\phi,\Sigma,\mu_{0},\mu_{1})=\frac{1}{1+exp(-\Theta^{T}x)}
\]

\end_inset

where 
\begin_inset Formula $\Theta$
\end_inset

is some appropriate function of the remainin parameters.
 This is the form that logistic regression used to model 
\begin_inset Formula $p(y=1|x)$
\end_inset

.
 When do we prefer one model over the other?
\end_layout

\begin_layout Standard
GDA makes 
\emph on
stronger
\emph default
 modeling assumptions than logistic regression does.
 It turns out that when these assumptions are correct, GDA will find better
 fits to the data.
 This is especially the case when 
\begin_inset Formula $p(x|y)$
\end_inset

 is gaussian.
 But by making weaker assumptions, logistic regression is more 
\emph on
robust
\emph default
 and less sensitive to wrong assumptions than GDA.
 So if we assumed that 
\begin_inset Formula $p(y|x)$
\end_inset

 is logistic and 
\begin_inset Formula $x|y\sim Poisson$
\end_inset

 then GDA may not do well on such non-Gaussian data.
 GDA makes more assumptions and is more data-efficient when the modeling
 assumptions are (at least approximately) correct.
 Logistic regression makes weaker assumptions and is more robust.
 Especially in non-Gaussian data, logistic regression will do better than
 GDA.
\end_layout

\begin_layout Subsection
Naive Bayes
\end_layout

\begin_layout Standard
GDA works with continuous, real-valued vectors.
 This is a different learning algorithm where the 
\begin_inset Formula $x_{i}$
\end_inset

's are discrete-valued.
 To demonstrate, assume we're gonna build an spam-filter.
\end_layout

\begin_layout Standard
We will represent an email via a feature vector whose length is equal to
 the length of the dictionary.
 If an email contains word number 
\begin_inset Formula $i$
\end_inset

 in the dictionary, then 
\begin_inset Formula $x_{i}=1$
\end_inset

, otherwise 
\begin_inset Formula $x_{i}=0$
\end_inset

.
 The words in the feature vector are called 
\series bold
vocabulary
\series default
.
 We want to model 
\begin_inset Formula $p(x|y)$
\end_inset

.
 This means that given spam or not spam, how possible is it that word 
\begin_inset Formula $i$
\end_inset

 is going to show up.
 To model 
\begin_inset Formula $p(x|y)$
\end_inset

 we're going to make a very strong assumption.
 We will assume that the 
\begin_inset Formula $x_{i}$
\end_inset

's are conditionally independent given 
\begin_inset Formula $y$
\end_inset

.
 This assumption is called the 
\series bold
Naive Bayes (NB) assumption
\series default
 and the resulting algorithm is called the 
\series bold
Naive Bayes classifier.
 
\series default
This means that when we have 
\begin_inset Formula $y=1$
\end_inset

, the knowledge of 
\begin_inset Formula $x_{2987}$
\end_inset

makes no effect on the knowledge of 
\begin_inset Formula $x_{19}$
\end_inset

.
 This can also be written as 
\begin_inset Formula $p(x_{2987}|y)=p(x_{2987}|y,x_{19})$
\end_inset

 (this is different from saying they are independet what would mean 
\begin_inset Formula $p(x_{2987})=p(x_{2987}|x_{19})$
\end_inset

).
 
\end_layout

\begin_layout Standard
We now have 
\begin_inset Formula $p(x_{1},\dots,x_{n}|y)=\prod_{i=1}^{n}p(x_{i}|y)$
\end_inset


\end_layout

\begin_layout Standard
Note that even though we made a strong assumptions, NB still works well
 on many problems.
 
\end_layout

\begin_layout Standard
Our model is parameterized by 
\begin_inset Formula $\phi_{i|y=1}=p(x_{i}=1|y=1)$
\end_inset

, 
\begin_inset Formula $\phi_{i|y=0}=p(x_{i}=1|y=0)$
\end_inset

 and 
\begin_inset Formula $\phi_{y}=p(y=1)$
\end_inset

.
 When we write down the joint likelihood of the data we get
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(\phi_{y},\phi_{i|y=0},\phi_{i|y=1})=\prod_{i=1}^{m}p(x^{(i)},y^{(i)})
\]

\end_inset

Maximizing this results in the maximum likelihood estimates:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j|y=1}=\frac{\sum_{i=1}^{m}1\{x_{j}^{(i)}=1\land y^{(i)}=1\}}{\sum_{i=1}^{m}1\{y^{(i)}=1\}}
\]

\end_inset

(How many times does the word show up in spam email in contrast to all spam
 email?)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j|y=0}=\frac{\sum_{i=1}^{m}1\{x_{j}^{(i)}=1\land y^{(i)}=0\}}{\sum_{i=1}^{m}1\{y^{(i)}=0\}}
\]

\end_inset

(How many times does the word show up in non-spam email in contrast to all
 non-spam email?)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{y}=\frac{\sum_{i=1}^{m}1\{y^{(i)}=1\}}{m}
\]

\end_inset


\end_layout

\begin_layout Standard
Now that we have fit all our data, to make a prediction we simply have to
 calculate
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(y=1|x)=\frac{p(x|y=1)p(y=1)}{p(x)}=\frac{(\prod_{i=1}^{n}p(x_{i}|y=1))p(y=1)}{(\prod_{i=1}^{n}p(x_{i}|y=1))p(y=1)+(\prod_{i=1}^{n}p(x_{i}|y=0))p(y=0)}=\frac{(\prod_{i=1}^{n}\phi_{i|y=1})\phi_{y}}{(\prod_{i=1}^{n}\phi_{i|y=1})\phi_{y}+(\prod_{i=1}^{n}\phi_{i|y=0})(1-\phi_{y})}
\]

\end_inset


\end_layout

\begin_layout Standard
Note that even though this example was with binary values, the generalization
 to a case where 
\begin_inset Formula $x_{i}$
\end_inset

can take on values in {1,2,...,k} is straightforward.
 We could just model 
\begin_inset Formula $p(x_{i}|y)$
\end_inset

 as multinomial rather than as Bernoulli.
 It is in practice quite common to 
\series bold
discretize
\series default
 and then use Naive Bayes.
 When original, contiuous-valued attributes are not well-modeled by a multivaria
te normal distribution, discretizing the features and using Naive Bayes
 will often result in a better classifier.
 
\end_layout

\begin_layout Subsubsection
Laplace smoothing 
\end_layout

\begin_layout Standard
Assume a word our algorithm has never seen shows up.
 Because it has not seen it before it will decide the probability to be
 zero for spam and non-spam mail what will will lead to 
\begin_inset Formula $\frac{0}{0}$
\end_inset

 division: Our algorithm doesn't know how to predict this case.
 To take account of such new words we can use 
\series bold
Laplace smoothing: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j}=\frac{(\sum_{i=1}^{m}1\{z^{(i)}=1\})+1}{m+k}
\]

\end_inset

 where k is the maximum value z can take on.
 Returning to our Naive Bayes classifier with Laplace smoothing we get:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j|y=1}=\frac{\sum_{i=1}^{m}1\{x_{j}^{(i)}=1\land y^{(i)}=1\}+1}{\sum_{i=1}^{m}1\{y^{(i)}=1\}+2}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{j|y=0}=\frac{\sum_{i=1}^{m}1\{x_{j}^{(i)}=1\land y^{(i)}=0\}+1}{\sum_{i=1}^{m}1\{y^{(i)}=0\}+2}
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Event models for text classification
\end_layout

\begin_layout Standard
We'll look at one more model for text classification.
 In the last chapter we assumed that first it is randomly determined whether
 the next mail is from a spammer or a non-spammer.
 Then the person sending the mail runs through the dictionary deciding whether
 to use or not use the word 
\begin_inset Formula $i$
\end_inset

 according to the probabilities 
\begin_inset Formula $\phi_{i|y}$
\end_inset

.
 Now we take a look at the 
\series bold
multinomial event model.
\end_layout

\begin_layout Standard
\begin_inset Formula $x_{i}$
\end_inset

will now denote the identitiy of the 
\begin_inset Formula $i$
\end_inset

-th word in the email.
 Thus, 
\begin_inset Formula $x_{i}$
\end_inset

can now take on values from 1 to 
\begin_inset Formula $|V|$
\end_inset

 where 
\begin_inset Formula $V$
\end_inset

 is the dictionary.
 
\begin_inset Formula $(1,17,...)$
\end_inset

 means the first word in the mail is the first from the dictionary, the
 second word is the 17-th from the dictionary, etc.
 Now again, it is first decided whether it is a spam mail or not.
 Then, the sender of the email writes the email by generating 
\begin_inset Formula $x_{1}$
\end_inset

from some multinomial distribution over words, then chooses 
\begin_inset Formula $x_{2}$
\end_inset

the same way but independently, etc.
 The overall probability of a message is then given by 
\begin_inset Formula $p(y)\prod_{i=1}^{n}p(x_{i}|y)$
\end_inset

.
 This looks similar as before but the terms in the formula mean now different
 things and in particular, 
\begin_inset Formula $x_{i}|y$
\end_inset

 is now multinomial, rather than Bernoulli.
 
\end_layout

\begin_layout Standard
The parameters are 
\begin_inset Formula $\phi_{y}=p(y)$
\end_inset

, 
\begin_inset Formula $\phi_{i|y=1}=p(x_{j}=i|y=1)$
\end_inset

,
\begin_inset Formula $\phi_{i|y=0}=p(x_{j}=i|y=0)$
\end_inset


\end_layout

\begin_layout Standard
The likelihood is then given by 
\end_layout

\begin_layout Standard
\SpecialChar ligaturebreak

\begin_inset Formula 
\[
L(\phi,\phi_{i|y=1},\phi_{i|y=0})=\prod_{i=1}^{m}(\prod_{j=1}^{n_{i}}p(x_{j}^{(i)}|y;\phi_{i|y=1};\phi_{i|y=0}))p(y^{(i)}|\phi).
\]

\end_inset

Maximizing this yield the maximum likelihhod estimates for the parameters:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{k|y=1}=\frac{\sum_{i=1}^{m}\sum_{j=1}^{n_{i}}1\{x_{j}^{(i)}=k\land y^{(i)}=1\}}{\sum_{i=1}^{m}1\{y^{(i)}=1\}n_{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{k|y=0}=\frac{\sum_{i=1}^{m}\sum_{j=1}^{n_{i}}1\{x_{j}^{(i)}=k\land y^{(i)}=0\}}{\sum_{i=1}^{m}1\{y^{(i)}=0\}n_{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{y}=\frac{\sum_{i=1}^{m}1\{y^{(i)}=1\}}{m}
\]

\end_inset

We can also apply Laplace smoothing (what is needed in practice for good
 performance):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{k|y=1}=\frac{\sum_{i=1}^{m}\sum_{j=1}^{n_{i}}1\{x_{j}^{(i)}=k\land y^{(i)}=1\}+1}{\sum_{i=1}^{m}1\{y^{(i)}=1\}n_{i}+|V|}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi_{k|y=0}=\frac{\sum_{i=1}^{m}\sum_{j=1}^{n_{i}}1\{x_{j}^{(i)}=k\land y^{(i)}=0\}+1}{\sum_{i=1}^{m}1\{y^{(i)}=0\}n_{i}+|V|}
\]

\end_inset


\end_layout

\begin_layout Section
Support Vector Machines
\end_layout

\begin_layout Subsection
Margins: Intuition
\end_layout

\begin_layout Standard
Consider logistic regression.
 We would predict 
\begin_inset Quotes eld
\end_inset

1
\begin_inset Quotes erd
\end_inset

 on an input 1 iff 
\begin_inset Formula $h_{\Theta}(x)\ge0.5$
\end_inset

 or equivlently iff 
\begin_inset Formula $\Theta^{T}x\ge0$
\end_inset

.
 The larger 
\begin_inset Formula $\Theta^{T}x$
\end_inset

 is, the higher our degree of 
\begin_inset Quotes eld
\end_inset

confidence
\begin_inset Quotes erd
\end_inset

 that the label is +1+.
 Informally we could also say, that 
\begin_inset Formula $y=1$
\end_inset

 if 
\begin_inset Formula $\Theta^{T}x\gg0$
\end_inset

.
 Similarly we could say that 
\begin_inset Formula $y=0$
\end_inset

 if 
\begin_inset Formula $\Theta^{T}x\ll0$
\end_inset

.
 Therefore we could say informally that we found a good fit if we can find
 
\begin_inset Formula $\Theta$
\end_inset

so that 
\begin_inset Formula $\Theta^{T}x\gg0$
\end_inset

 whenever 
\begin_inset Formula $y^{(i)}=1$
\end_inset

, and 
\begin_inset Formula $\Theta^{T}x\ll0$
\end_inset

 whenever 
\begin_inset Formula $y^{(i)}=0.$
\end_inset


\end_layout

\begin_layout Standard
For another type of intuition, consider the following figure where x's represent
 positive training examples and y's negative training examples.
 The line is also called 
\series bold
separating hyperplane
\series default
.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename margins_intuition.png

\end_inset


\end_layout

\begin_layout Standard
Making a prediction for A should be much easier and more confident than
 one for C.
 It would be nice to have a classifier that allows us to make correct and
 confident predictions on the training examples.
 
\end_layout

\begin_layout Subsection
Notation
\end_layout

\begin_layout Standard
To make things easier we'll consider a linear classifier for a binary classifica
tion problem with labels 
\begin_inset Formula $y\in\{-1,1\}$
\end_inset

.
 We will also use parameters 
\begin_inset Formula $w,b$
\end_inset

 and write our classifier as 
\begin_inset Formula $h_{w,b}(x)=g(w^{T}x+b)$
\end_inset

 where 
\begin_inset Formula $g(z)=1$
\end_inset

if 
\begin_inset Formula $z\ge0$
\end_inset

and 0 otherwise.
 So, 
\begin_inset Formula $b$
\end_inset

 take the role of 
\begin_inset Formula $\Theta_{0}$
\end_inset

and 
\series bold

\begin_inset Formula $w$
\end_inset

 
\series default
the role of 
\begin_inset Formula $[\Theta_{1},\dots,\Theta_{n}]^{T}$
\end_inset

.
\end_layout

\begin_layout Subsection
Functional and geometric margins
\end_layout

\begin_layout Standard
We define the 
\series bold
functional margin
\series default
 of 
\begin_inset Formula $(w,b)$
\end_inset

 with respect to a training example 
\begin_inset Formula $i$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\gamma}^{(i)}=y^{(i)}(w^{T}x+b)
\]

\end_inset

Note, that when 
\begin_inset Formula $y^{(i)}$
\end_inset

is 1, then for the functional margin to be large, 
\begin_inset Formula $(w^{T}x+b)$
\end_inset

 needs to be a large positive number.
 Similar for 
\begin_inset Formula $y^{(i)}=-1$
\end_inset

it needs to be a large negative number.
 Moreover, if 
\begin_inset Formula $y^{(i)}(w^{T}x+b)>0$
\end_inset

, then our prediction on this example is correct.
 The problem here is that scaling our 
\begin_inset Formula $w,b$
\end_inset

 does not change our hypothesis 
\begin_inset Formula $h_{w,b}$
\end_inset

since it only depends on the sign.
 But scaling affects our functional margin.
 It might make sense to normalize this so we can't exploit this freedom.
 We'll come back later to this.
 For a whole training set 
\begin_inset Formula $S$
\end_inset

 we set the overall function margin to be the smallest of the functional
 margins:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\gamma}=min\hat{\gamma}^{^{(i)}}
\]

\end_inset

Let's talk about 
\series bold
geometric margins
\series default
.
 Consider the following figure: 
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename geometrix_margin.png

\end_inset


\end_layout

\begin_layout Standard
We want to be able to find the value of 
\begin_inset Formula $\gamma^{(i)}$
\end_inset

.
 We know that B is given by 
\begin_inset Formula $x^{(i)}-\gamma^{(i)}\cdot w/||w||$
\end_inset

.
 It also has to satisfy the equation of the decision boundary 
\begin_inset Formula $w^{T}x+b=0$
\end_inset

.
 Hence,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
w^{T}\left(x^{(i)}-\gamma^{(i)}\frac{w}{||w||}\right)+b=0
\]

\end_inset

Solving for 
\begin_inset Formula $\gamma^{(i)}$
\end_inset

 yields
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y^{(i)}=\left(\frac{w}{||w||}\right)^{T}x^{(i)}+\frac{b}{||w||}
\]

\end_inset

More generally, to consider negative 
\begin_inset Formula $y$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y^{(i)}=y^{(i)}\left(\left(\frac{w}{||w||}\right)^{T}x^{(i)}+\frac{b}{||w||}\right)
\]

\end_inset

Now scaling 
\begin_inset Formula $w,b$
\end_inset

 does not change the margin.
 Again we define the geometric margin as the smallest of the geometric margins
 of the training set.
 
\end_layout

\begin_layout Subsection
The optimal margin classifier
\end_layout

\begin_layout Standard
It seems that a good way is to find a decision boundary that maximizes the
 margin.
 For this (and now), we'll assume that our data is linear separable.
 This poses the following optimization problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
max_{\gamma,w,b}\gamma
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
s.t.\ y^{(i)}(w^{T}x^{(i)}+b)\ge\gamma,\ i=1,\dots,m
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
||w||=1
\]

\end_inset

We want to maximize 
\begin_inset Formula $\gamma$
\end_inset

.
 The last constraint ensures that the functional margin equals the geometric
 margin.
 Solving this problem will result with 
\begin_inset Formula $(w,b)$
\end_inset

 with the largest possible geometric margin.
 This problem is quite hard to solve, so we're going to transform it a bit:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
max_{\gamma,w,b}\frac{\hat{\gamma}}{||w||}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
s.t.\ y^{(i)}(w^{T}x^{(i)}+b)\ge\hat{\gamma},\ i=1,\dots,m
\]

\end_inset

We can do this because of 
\begin_inset Formula $\gamma=\frac{\hat{\gamma}}{||w||}$
\end_inset

.
 This is still a bit nasty, so we keep going with a bit of scaling.
 Since we are now allowed to scale we can set 
\begin_inset Formula $\hat{\gamma}=1$
\end_inset

 and rescale 
\begin_inset Formula $(w,b)$
\end_inset

what leads to:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
min_{\gamma,w,b}\frac{1}{2}||w||^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
s.t.\ y^{(i)}(w^{T}x^{(i)}+b)\ge1,\ i=1,\dots,m
\]

\end_inset

This can be solved efficiently and is called the 
\series bold
optimal margin classifier
\series default
.
 
\end_layout

\end_body
\end_document
