#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Machine Learning Assignment #3 
\end_layout

\begin_layout Title
Probability theory review
\end_layout

\begin_layout Author
Simon Kafader, 15-114-945
\end_layout

\begin_layout Enumerate
If X and Y are independent, then we know that 
\begin_inset Formula $E(XY)=E(X)E(Y)$
\end_inset

.
 Thus, 
\begin_inset Formula $Cov(X,Y)=E(XY)−E(X)E(Y)=E(X)E(Y)−E(X)E(Y)=0$
\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Now let X be uniformly distributed in [−1, 1] and let 
\begin_inset Formula $Y=X^{2}$
\end_inset

.
 Clearly, X and Y are dependent, but
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
COV[X,Y]=COV[X,X^{2}]=E[X\cdot X^{2}]-E[X]E[X^{2}]=E[X^{3}]-E[X]E[X^{2}]=0-0\cdot E[X^{2}]=0
\]

\end_inset


\end_layout

\begin_layout Standard
So even though the covariance here is zero, the variables are not independent.
\end_layout

\begin_layout Standard
b) 
\end_layout

\begin_layout Standard
From the definition of the covariance matrix we can easily see that it must
 be a square matrix: 
\begin_inset Formula $V(X)=(Cov(X_{i},X_{j}))_{i,j=1,...,n}$
\end_inset


\end_layout

\begin_layout Standard
Proof that it is symmetric:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Var[X]^{\top}=E[(X-E[X])(X-E[X])^{\top}]^{\top}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=E[((X-E[X])(X-E[X])^{\top})^{\top}]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=E[(X-E[X])(X-E[X])^{\top}]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=Var[X]
\]

\end_inset


\begin_inset Newline newline
\end_inset

Proof of semi-definite:
\end_layout

\begin_layout Standard
For vectors 
\begin_inset Formula $x_{i}=(x_{i1},...,x_{ik})^{\top},i=1,...,n$
\end_inset

, the covariance matrix is 
\begin_inset Formula $Q=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})(x_{i}-\bar{x})^{\top}\,.$
\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $y\in\mathbb{R}^{k}$
\end_inset

be a nonzero vector:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y^{\top}Qy=y^{\top}\left(\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})(x_{i}-\bar{x})^{\top}\right)y
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{n}\sum_{i=1}^{n}y^{\top}(x_{i}-\bar{x})(x_{i}-\bar{x})^{\top}y
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{n}\sum_{i=1}^{n}((x_{i}-\bar{x})^{\top}y)^{\top}((x_{i}-\bar{x})^{\top}y)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=\frac{1}{n}\sum_{i=1}^{n}\left((x_{i}-\bar{x})^{\top}y\right)^{2}\geq0\,.
\]

\end_inset


\end_layout

\begin_layout Standard
Therefore Q is always positive semi-definite.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
2.)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E(Z)=E(AX+Y)
\]

\end_inset


\end_layout

\begin_layout Standard
We know that the expectation is a linear function.
 This means we can take the matrix out and split it
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E(AX+Y)=AE(X)+E(Y)=A\cdot0+\mu=\mu
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Cov[Z]=Cov(Z,Z)=Cov(AX+Y,AX+Y)
\]

\end_inset


\end_layout

\begin_layout Standard
Now since we know that the covariance is bilinear and symmetric we can split
 it up as follows
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Cov(AX+Y,AX+Y)=Cov(AX,AX)+2\cdot Cov(AX,Y)+Cov(Y,Y)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
=ACov(X,X)A^{\top}+2ACov(X,Y)+Cov(Y,Y)
\]

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent random variables, we know that 
\begin_inset Formula $Cov(X,Y)=0$
\end_inset

.
 Therefore
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Cov[Z]=AIA^{\top}+2A0+\sigma I=AA^{\top}+\sigma I
\]

\end_inset


\end_layout

\begin_layout Standard
3.) The probability of Thomas finding Viktor in this last bar is exactly
 
\begin_inset Formula $(2/3)*(1/5)=2/15$
\end_inset

.
 This is because informally Viktor decides beforehand if and if so to which
 party he goes.
 This makes the stated probability.
 Now because this is decided beforehand, getting new information does not
 change
\end_layout

\begin_layout Standard
anything about the probability since it is already fixed.
 Therefore it is 
\begin_inset Formula $2/15$
\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
4.)
\end_layout

\begin_layout Standard
We start with taking a constant out of the integral:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\int_{x\in\mathbb{R^{n}}}e^{-\frac{1}{2}x^{\top}Ax-x^{\top}b-c}=e^{-c}\int_{x\in\mathbb{R^{n}}}e^{-\frac{1}{2}x^{\top}Ax-x^{\top}b}
\]

\end_inset

 Now let 
\begin_inset Formula $X$
\end_inset

 be an n-dimensional random variable and A its covariance matrix.
 Now from the hint in the excercise we know that it is normally (gaussian)
 distributed.
 Since 
\begin_inset Formula $A$
\end_inset

 is positive definite the multivariate normal distribution is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{align}f_{\mathbf{X}}(x_{1},\ldots,x_{n}) & =\frac{\exp\left(-\frac{1}{2}({\mathbf{x}}-{\boldsymbol{\mu}})^{\mathrm{\top}}{\boldsymbol{\Sigma}}^{-1}({\mathbf{x}}-{\boldsymbol{\mu}})\right)}{\sqrt{(2\pi)^{n}|\boldsymbol{\Sigma}|}}\\[5pt]
\\
\\
\\
\end{align}
\]

\end_inset


\end_layout

\begin_layout Standard
When we use it for our remaining integral with 
\begin_inset Formula $\mu=x-b$
\end_inset

 we get the following equatation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\int_{x\in\mathbb{R^{n}}}e^{-\frac{1}{2}x^{\top}Ax-x^{\top}b-c}=e^{-c}\int_{x\in\mathbb{R^{n}}}e^{-\frac{1}{2}x^{\top}Ax-x^{\top}b}=\frac{\exp\left(-c-\frac{1}{2}b{}^{\top}A^{-1}b\right)}{\sqrt{(2\pi)^{n}|\boldsymbol{A}|}}=\frac{(2\pi)^{\frac{n}{2}}|A|^{-\frac{1}{2}}}{e^{c-\frac{1}{2}b^{\top}A^{-1}b}|}
\]

\end_inset


\end_layout

\begin_layout Standard
5.) As we've seen in the lecture we prefer to use the log likelihood for
 maximising such functions.
 Therefore this log likelihood here will be:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
l(\lambda)=\sum_{i=1}^{n}(X_{i}log\lambda-\lambda-log(X_{i}!))=log(\lambda)\sum_{i=1}^{n}(X_{i}-n\lambda)-\sum_{i=1}^{n}log(X_{i}!)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
l'(\lambda)=\frac{1}{\lambda}\sum_{i=1}^{n}x_{i}-n=0
\]

\end_inset


\end_layout

\begin_layout Standard
This implies that the estimate should be 
\begin_inset Formula $\lambda=\overline{X}$
\end_inset


\end_layout

\begin_layout Standard
6.)
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
Let's start at the right side:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(x_{1)}\cdot f(x_{2}|\,x_{1})\cdot\dots\cdot f(x_{n}|\,x_{1},\dots x_{n-1})=f(x_{1})\cdot\frac{f(x_{1},x_{2})}{f(x_{1})}\cdot\frac{f(x_{1},x_{2},x_{3})}{f(x_{1},x_{2})}\cdot\dots\cdot\frac{f(x_{1},\dots,\,x_{n})}{f(x_{1},\dots,x_{n-1})}
\]

\end_inset


\end_layout

\begin_layout Standard
Now we can strike away the numerator i with the denominator i+1 what only
 leaves one term:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(x_{1})\cdot\frac{f(x_{1},x_{2})}{f(x_{1})}\cdot\frac{f(x_{1},x_{2},x_{3})}{f(x_{1},x_{2})}\cdot\dots\cdot\frac{f(x_{1},\dots,\,x_{n})}{f(x_{1},\dots,x_{n-1})}=f(x_{1},\dots,x_{n})
\]

\end_inset


\end_layout

\end_body
\end_document
